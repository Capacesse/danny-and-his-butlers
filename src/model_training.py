# -*- coding: utf-8 -*-
"""model_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h8gUi92JUXEODnC9EkvogAK9--TBnGjD

## README
- this is to build the pipeline for models that are used to classify the dataset
- the functions here won't take the preprocessed dataset that is not processed by the feature engineer
- the only file that it requires is the dataframe provided by feature engineer (i.e. the get_unified_data will be called in the main pipeline, to create a unified dataframe and this dataframe will be passed to the functions here)
- but get_unified_data is still called here solely for the purpose of preparing dataframe for testing
"""

#PACKAGES
# please download this in the file that you have to run the function that is in this module
# !pip install nltk
# !pip install vaderSentiment
# from google.colab import drive
# drive.mount('/content/drive')
# from google.colab import userdata
import pandas as pd
# import sys
import os
import numpy as np
import joblib
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
# feature_engineering_path = '/content/drive/MyDrive/TikTok_Tech_Jam/src'
# if feature_engineering_path not in sys.path:
#     sys.path.append(feature_engineering_path)
# from feature_engineering import get_unified_data
# from data_preprocessing import preprocess_data
import lightgbm as lgb
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report
from sklearn.base import clone
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, accuracy_score, balanced_accuracy_score, cohen_kappa_score, classification_report, auc
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import seaborn as sns


########
## 1. ## we already have the get_unified_data from feature_engineering to import the unified data (pd.DataFrame)
########
        ## - user has to call this function to get the dataframe before the code in this module can be used



########
## 2. ## the dataframe from step 1. will be passed into here to generate the pseuo_labels which is according to the business rules created by human
########
def generate_pseudo_labels(df: pd.DataFrame) -> pd.DataFrame:
    """
    Applies a set of high-confidence, hierarchical rules to create a pseudo-label
    for a portion of the dataset (Refinement 2).
    """
    df_labeled = df.copy()

    # --- Define all conditions using boolean Series ---

    # 1. Advertisement Condition (highest priority) - Refinement 2 (Adjusted)
    ad_condition = (
        (df_labeled['has_url'] == 1) |
        # Adjusted condition for high sentiment and exclamation points, added word count
        ((df_labeled['has_url'] == 0) & (df_labeled['sentiment_score'] > 0.9) & (df_labeled['exclamation_count'] > 1) & (df_labeled['word_count'] > 5))
    )


    # 2. Rant Without Visit Condition (Refined) - Refinement 2
    rant_condition = ((df_labeled['is_zero_visit'] == 1) & (df_labeled['sentiment_score'] < -0.6)) | \
                     ((df_labeled['is_zero_visit'] == 1) & (df_labeled['sentiment_score'] < -0.1)) | \
                     ((df_labeled['is_zero_visit'] == 1) & (df_labeled['char_count'] <= 5))


    # 3. Irrelevant Condition (Refined again) - Refinement 2
    # Further refined word count, char count, and sentiment/rating correlations
    irrelevant_condition = ((df_labeled['word_count'] < 4) | (df_labeled['char_count'] < 10)) | \
                           ((df_labeled['word_count'] <= 3) & (df_labeled['question_mark_count'] > 1)) | \
                           ((df_labeled['rating'] > 3) & (df_labeled['sentiment_score'] < -0.3)) | \
                           ((df_labeled['rating'] < 3) & (df_labeled['sentiment_score'] > 0.3)) | \
                           ((df_labeled['rating'] == 3) & ((df_labeled['sentiment_score'] < -0.1) | (df_labeled['sentiment_score'] > 0.1)))


    # 4. Valid Review Condition (Sophisticated correlation checks - refined from Refinement 1) - Refinement 2
    valid_review_condition_positive = (df_labeled['rating'] >= 4) & (df_labeled['sentiment_score'] > 0.4) & (df_labeled['word_count'] >= 10)
    valid_review_condition_negative_valid = (df_labeled['rating'] <= 2) & (df_labeled['sentiment_score'] < -0.4) & (df_labeled['word_count'] >= 10)
    valid_review_condition_neutral_positive = (df_labeled['rating'].isin([3, 4])) & \
                                              (df_labeled['sentiment_score'] > 0.1) & \
                                              (df_labeled['sentiment_score'] <= 0.4) & \
                                              (df_labeled['word_count'] >= 10)
    valid_review_condition_neutral_negative = (df_labeled['rating'].isin([2, 3])) & \
                                              (df_labeled['sentiment_score'] >= -0.4) & \
                                              (df_labeled['sentiment_score'] <= -0.1) & \
                                              (df_labeled['word_count'] >= 10)

    valid_review_condition = valid_review_condition_positive | valid_review_condition_negative_valid | \
                             valid_review_condition_neutral_positive | valid_review_condition_neutral_negative


    # --- Apply rules using np.select to ensure mutual exclusivity ---

    # Define the conditions and their corresponding labels in a hierarchical order
    conditions = [ad_condition, rant_condition, irrelevant_condition, valid_review_condition]
    labels = ['Advertisement', 'Rant Without Visit', 'Irrelevant', 'Valid Review']

    # Apply the conditions; the first condition that is True will be assigned
    df_labeled['pseudo_label'] = np.select(conditions, labels, default=None)

    return df_labeled


########
## 3. ## we will separate the df into X (independent variables) and y (dependent variables) to be trained for the labelling model
########
def prepare_data_for_training(labeled_df: pd.DataFrame):
    """
    Filters out unlabeled data and separates the DataFrame into X and y to train the initial model.
    """
    # Filter for rows that have a pseudo_label (i.e., not NaN)
    labeled_data_only = labeled_df.dropna(subset=['pseudo_label'])

    # Identify all feature columns by dropping the 'pseudo_label' column
    feature_columns = labeled_data_only.columns.drop('pseudo_label').tolist()

    # Define X (the feature matrix)
    X = labeled_data_only[feature_columns]

    # Define y (the target variable)
    y = labeled_data_only['pseudo_label']

    print("\nTraining data prepared.")
    print(f"Number of successfully pseudo-labeled reviews: {len(labeled_data_only)}")
    print(f"Shape of X (Features): {X.shape}")
    print(f"Shape of y (Labels): {y.shape}")
    print(f"Distribution of pseudo-labels:\n{y.value_counts()}")

    return X, y

########
## 4. ## Now, we will use the X and y returned to train and save the model(a file will be saved into the `models` folder in drive under the main folder)
########
def train_and_save_baseline_model(X, y, model_save_path='/content/drive/MyDrive/TikTok_Tech_Jam/models/classifier_model.pkl'):
    """
    Splits the data, trains a Logistic Regression model, and saves it
    to a specified path (defaulting to a Google Drive location).
    """
    # Split the labeled data into a training set and a testing set
    # This is a critical step for proper model evaluation.
 ##try if can stratify = y, if least class only has 1 member, then remove the stratify = y but raise a warning
    try:
      X_train, X_test, y_train, y_test = train_test_split(
          X, y,
          test_size=0.2,
          random_state=42,
          stratify=y
      )
    except ValueError:
      #if there is only 1 member in the least populated class, it will raise
      #ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
      if y.value_counts().min() == 1:
        print("""
        WARNING: The least populated class(es) in y has(have) only 1 member, which is too few.
        The minimum number of groups for any class cannot be less than 2 if want to keep the proportion of the classes in train and test dataset.
        stratify = y has been removed in train_test_split for this case.
        Please add more values for the least populated class(es) and run the function again if you want to keep the proportion of the classes in train and test dataset.
        """)
        X_train, X_test, y_train, y_test = train_test_split(
          X, y,
          test_size=0.2,
          random_state=42
      )
      else:
        raise
    except:
      if len(y.value_counts().index) < 4:
        print("""
        WARNING: There is one or more classes has no instance at all.
        stratify = y has been removed in train_test_split for this case.
        Please add more values for the class(es) and run the function again if you want to keep the proportion of the classes in train and test dataset.
        """)
        X_train, X_test, y_train, y_test = train_test_split(
          X, y,
          test_size=0.2,
          random_state=42
      )
      else:
        raise

    print("\nTraining a Logistic Regression baseline model...")

    # Initialize and train the model
    model = LogisticRegression(max_iter=1000)
    model.fit(X_train, y_train)

    # Evaluate the model on the test set (optional in this function, but good practice)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"Model trained successfully.")
    print(f"Accuracy on test set: {accuracy:.4f}") # Note: This accuracy is on the split within the function


    # Create the models directory if it doesn't exist
    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)

    # Save the trained model using joblib
    joblib.dump(model, model_save_path)

    print(f"Baseline model saved to: {model_save_path}")

########
## 5. ## run the prediction pipeline to generate labels for the entire dataset
########

def run_prediction_pipeline(unified_df: pd.DataFrame, model_path: str):
    """
    Runs the full end-to-end pipeline to generate predictions for every
    review in the dataset using a hybrid rule-based and ML approach.

    Args:
        unified_df (pd.DataFrame): The unified feature matrix.
        model_path (str): The path to the trained model file.

    Returns:
        pd.DataFrame: A DataFrame with all reviews and their predicted labels.
    """
    print("--- Starting Hybrid Prediction Pipeline ---")

    # Step 1: Run Feature Engineering & Rule-based Pseudo-labeling
    # We will use your existing function to apply the rules.
    print("1. Running Feature Engineering and Rule-based labeling...")
    try:
      df_with_labels = generate_pseudo_labels(unified_df.copy())
      df_with_labels = df_with_labels.rename(columns = {"pseudo_label": "predicted_label"})
    except Exception as e:
      print(f"Feature Engineering failed: {e}")
      return None

    # Step 2: Identify instances that still need to be classified by the model
    # These are the instances where the pseudo-label is still None
    # We will use .loc to avoid a SettingWithCopyWarning
    df_to_predict = df_with_labels.loc[df_with_labels['predicted_label'].isnull()].copy()

    if df_to_predict.empty:
        print("All instances were successfully labeled by the rules. No model prediction needed.")
        return df_with_labels  #can save time as well

    print(f"   Rules labeled {len(df_with_labels) - len(df_to_predict)} instances.")
    print(f"   {len(df_to_predict)} instances remain for model prediction.")

    # Step 3: Load the pre-trained model
    print("2. Loading the pre-trained model...")
    try:
        model_ensemble = joblib.load(model_path)
    except FileNotFoundError:
        print(f"Error: Trained model not found at {model_path}")
        return None

    # Step 4: Make predictions on the remaining subset
    print("3. Generating predictions on the remaining instances...")
    try:
        # Separate features from any existing labels
        features_only_df = df_to_predict.drop(columns=['predicted_label'], errors='ignore')

        # Make predictions using the loaded ensemble model
        predictions = model_ensemble.predict(features_only_df)

        # Assign the predictions to the correct rows in the original dataframe
        df_with_labels.loc[df_with_labels['predicted_label'].isnull(), 'predicted_label'] = predictions

    except Exception as e:
        print(f"Prediction failed: {e}")
        return None

    print("--- Pipeline completed successfully. ---")
    return df_with_labels

########
## 6. ## split our data into train and test first, then split it into specific categories to train them based on their specialized model
########
def prepare_and_split_data(df: pd.DataFrame):
    """
    Separates the DataFrame into X and y, performs the final train/test split,
    and then separates the feature sets for the ensemble.

    Args:
        df (pd.DataFrame): The full DataFrame with features and predicted labels.

    Returns:
        A tuple of all correctly aligned train and test sets.
    """
    ## Filter out rows where predicted_label is None (just in case)
    df = df.dropna(subset = ["predicted_label"])
    y = df['predicted_label']
    X = df.drop(columns=['predicted_label'])

    # 1. Perform the train/test split on the full dataset first to maintain alignment

    ##try if can stratify = y, if least class only has 1 member, then remove the stratify = y but raise a warning
    try:
      X_train, X_test, y_train, y_test = train_test_split(
          X, y,
          test_size=0.2,
          random_state=42,
          stratify=y
      )
    except ValueError:
      #if there is only 1 member in the least populated class, it will raise
      #ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
      if y.value_counts().min() == 1:
        print("""
        WARNING: The least populated class(es) in y has(have) only 1 member, which is too few.
        The minimum number of groups for any class cannot be less than 2 if want to keep the proportion of the classes in train and test dataset.
        stratify = y has been removed in train_test_split for this case.
        Please add more values for the least populated class(es) and run the function again if you want to keep the proportion of the classes in train and test dataset.
        """)
        X_train, X_test, y_train, y_test = train_test_split(
          X, y,
          test_size=0.2,
          random_state=42
      )
      else:
        raise
    except:
      if len(y.value_counts().index) < 4:
        print("""
        WARNING: There is one or more classes has no instance at all.
        stratify = y has been removed in train_test_split for this case.
        Please add more values for the class(es) and run the function again if you want to keep the proportion of the classes in train and test dataset.
        """)
        X_train, X_test, y_train, y_test = train_test_split(
          X, y,
          test_size=0.2,
          random_state=42
      )
      else:
        raise


    # Identify the feature groups based on our data contract
    classic_features = ['rating', 'has_url', 'exclamation_count', 'question_mark_count',
                        'ellipsis_count', 'is_zero_visit', 'all_caps_word_count',
                        'capital_letter_percentage', 'word_count', 'char_count',
                        'sentiment_score']

    # TF-IDF features are all the words, excluding others
    tfidf_features = [col for col in X.columns if not col.startswith('embedding_dim_') and col not in classic_features]

    # Embedding features are the 384 dimensions
    embedding_features = [col for col in X.columns if col.startswith('embedding_dim_')]

    # 2. Separate the features for the train and test sets now that they are aligned
    X_classic_train = X_train[classic_features]
    X_tfidf_train = X_train[tfidf_features]
    X_embeddings_train = X_train[embedding_features]

    X_classic_test = X_test[classic_features]
    X_tfidf_test = X_test[tfidf_features]
    X_embeddings_test = X_test[embedding_features]

    return X_classic_train, X_classic_test, y_train, y_test, X_tfidf_train, X_tfidf_test, X_embeddings_train, X_embeddings_test




##############################################################
# FUNCTION TO TRANSFORM DATASET INTO 3 GROUPS FOR PREDICTING #
##############################################################
def transform_for_prediction(df: pd.DataFrame):  #will return 3 pd df
#the input df only has independent features
    # Identify the feature groups based on our data contract
    classic_features = ['rating', 'has_url', 'exclamation_count', 'question_mark_count',
                        'ellipsis_count', 'is_zero_visit', 'all_caps_word_count',
                        'capital_letter_percentage', 'word_count', 'char_count',
                        'sentiment_score']

    # TF-IDF features are all the words, excluding others
    tfidf_features = [col for col in df.columns if not col.startswith('embedding_dim_') and col not in classic_features]

    # Embedding features are the 384 dimensions
    embedding_features = [col for col in df.columns if col.startswith('embedding_dim_')]

    # 2. Separate the features for the train and test sets now that they are aligned
    X_classic = df[classic_features]
    X_tfidf = df[tfidf_features]
    X_embeddings = df[embedding_features]

    return X_classic, X_tfidf, X_embeddings
###############################################################


########
## 7. ## final model training and will save a model into the `models` file as well, in the final pipeline can call the model based on this file
########
def train_and_save_final_ensemble(X_classic_train, X_tfidf_train, X_embeddings_train,
                                 y_train, X_classic_test, X_tfidf_test,
                                 X_embeddings_test, y_test):
    """
    Builds, trains, evaluates, and saves the final production ensemble model
    using a correct cross-validation stacking approach to prevent data leakage.
    """
    print("\nTraining the final production ensemble model...")

    # --- Step 1: Define the models and initialize OOF arrays ---
    classic_model = LogisticRegression(random_state=42)
    tfidf_model = lgb.LGBMClassifier(random_state=42, n_estimators=100)
    embedding_model = RandomForestClassifier(n_estimators=100, random_state=42)
    final_meta_model = LogisticRegression(random_state=42)

    n_classes = len(y_train.unique())
    n_train_samples = len(y_train)

    # Initialize X_meta_train with the correct index from X_classic_train
    X_meta_train = pd.DataFrame(index=X_classic_train.index, columns=range(n_classes * 3))
    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    # --- Step 2: Generate Out-of-Fold (OOF) predictions for meta-training ---
    ##this is to prevent the base models to learn from the rules from the train data, then get the probability score for the train data again
    ##bcs this will cause the accuracy for their probability score to be very high because they have been exposed to that specific data before
    ##so, our last ensemble model can't really learn from the prob score that they return bcs it is meant to learn from the not so perfect prob score
    ##to find the relation between how these not so perfect prob score can lead to the correct result
    ##mainly just to prevent the base models to predict on the same dataset that it learnt on
    print("1. Generating Out-of-Fold predictions for meta-model training...")

    for fold, (train_idx, val_idx) in enumerate(kf.split(X_classic_train, y_train)):
        print(f"   Processing Fold {fold + 1}/5...")

        # Split the data for this fold
        X_classic_fold_train, X_classic_fold_val = X_classic_train.iloc[train_idx], X_classic_train.iloc[val_idx]
        X_tfidf_fold_train, X_tfidf_fold_val = X_tfidf_train.iloc[train_idx], X_tfidf_train.iloc[val_idx]
        X_embeddings_fold_train, X_embeddings_fold_val = X_embeddings_train.iloc[train_idx], X_embeddings_train.iloc[val_idx]
        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

        # Train models on the fold training data
        ##it will learn the rules from the 4folds
        classic_model.fit(X_classic_fold_train, y_fold_train)
        tfidf_model.fit(X_tfidf_fold_train, y_fold_train)
        embedding_model.fit(X_embeddings_fold_train, y_fold_train)

        # Predict on the held-out validation fold
        ##then apply the rule on the left 1 fold to generate the score for each class
        val_classic_preds = classic_model.predict_proba(X_classic_fold_val)
        val_tfidf_preds = tfidf_model.predict_proba(X_tfidf_fold_val)
        val_embedding_preds = embedding_model.predict_proba(X_embeddings_fold_val)

        # Concatenate predictions and ensure their index matches the validation index
        fold_meta_features = pd.concat([
            pd.DataFrame(val_classic_preds, index=y_fold_val.index),
            pd.DataFrame(val_tfidf_preds, index=y_fold_val.index),
            pd.DataFrame(val_embedding_preds, index=y_fold_val.index)
        ], axis=1)

        # Store the predictions in the meta-training dataset using the validation index
        ##store it in a meta dataset to be learnt by the meta model(when each of the model give these scores,what class it will likely to be)
        X_meta_train.loc[y_fold_val.index] = fold_meta_features.values


    # Re-train base models on the full training data for final prediction
    ##bcs the cross validation rules are only trained on 80% of the train data, so it cannot capture the relationship between all instances
    ##so not the best one
    classic_model.fit(X_classic_train, y_train)
    tfidf_model.fit(X_tfidf_train, y_train)
    embedding_model.fit(X_embeddings_train, y_train)

    # --- Step 3: Train the Final Meta-Model ---
    print("\n2. Training the final meta-model...")
    final_meta_model.fit(X_meta_train, y_train)

    # --- Step 4: Generate predictions for the test set ---
    print("3. Generating final predictions on the test set...")
    test_classic_preds = classic_model.predict_proba(X_classic_test)
    test_tfidf_preds = tfidf_model.predict_proba(X_tfidf_test)
    test_embedding_preds = embedding_model.predict_proba(X_embeddings_test)

    X_meta_test = pd.concat([
        pd.DataFrame(test_classic_preds),
        pd.DataFrame(test_tfidf_preds),
        pd.DataFrame(test_embedding_preds)
    ], axis=1)

    y_pred = final_meta_model.predict(X_meta_test)
    y_prob = final_meta_model.predict_proba(X_meta_test) # Get probabilities for ROC/AUC
    accuracy = accuracy_score(y_test, y_pred)

    print("\nFinal ensemble model trained successfully.")
    print(f"Final Model Accuracy on Test Set: {accuracy:.4f}")
    print("\nFinal Classification Report:")
    print(classification_report(y_test, y_pred, zero_division=0))

    # --- Step 5: Save the Entire Ensemble ---
    ensemble_pipeline = {
        'classic_model': classic_model,
        'tfidf_model': tfidf_model,
        'embedding_model': embedding_model,
        'final_meta_model': final_meta_model
    }

    final_model_path = '/content/drive/MyDrive/TikTok_Tech_Jam/models/final_production_ensemble_model.pkl'
    os.makedirs(os.path.dirname(final_model_path), exist_ok=True)
    joblib.dump(ensemble_pipeline, final_model_path)

    print(f"\nFinal production ensemble pipeline deployed and saved to: {final_model_path}")

    return X_classic_test, X_tfidf_test, X_embeddings_test, y_test, y_pred, y_prob # Return y_prob as well

########
## 8. ## this provide the model evaluation function that can be used to give the user about the information of the model
########

def perform_comprehensive_evaluation(y_test, y_pred, y_prob):
    """
    Performs comprehensive model evaluation including metrics, tables, and visualizations.

    Args:
        y_test (pd.Series): True labels for the test set.
        y_pred (np.ndarray): Predicted labels for the test set.
        y_prob (np.ndarray): Predicted probabilities for the test set.
    """
    print("\n--- Comprehensive Model Evaluation ---")

    # 1. Overall Metrics Table
    accuracy = accuracy_score(y_test, y_pred)
    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)
    kappa = cohen_kappa_score(y_test, y_pred)

    overall_metrics = {
        "Metric": ["Overall Accuracy", "Balanced Accuracy", "Kappa Score"],
        "Score": [accuracy, balanced_accuracy, kappa]
    }
    overall_metrics_df = pd.DataFrame(overall_metrics)

    print("\nOverall Model Metrics:")
    display(overall_metrics_df)


    # 2. Classification Report (already a table)
    print("\nClassification Report:")
    report = classification_report(y_test, y_pred, zero_division=0, output_dict=True)
    print(classification_report(y_test, y_pred, zero_division=0))

    # 3. Confusion Matrix (as a table)
    # Get unique labels from both true and predicted labels to ensure all classes are in the matrix
    all_labels = sorted(list(set(y_test) | set(y_pred)))
    cm = confusion_matrix(y_test, y_pred, labels=all_labels)
    cm_df = pd.DataFrame(cm, index=[f'True: {label}' for label in all_labels], columns=[f'Predicted: {label}' for label in all_labels])

    print("\nConfusion Matrix (Table Format):")
    display(cm_df)


    # 4. Confusion Matrix (Heatmap Visualization)
    print("\nConfusion Matrix (Heatmap Visualization):")
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=all_labels, yticklabels=all_labels)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()


    # 5. Visualize Precision, Recall, and F1-score per Class (Bar Chart)
    print("\nPrecision, Recall, and F1-score per Class Visualization:")

    # Extract metrics for plotting (excluding macro/weighted averages and accuracy)
    metrics_data = {}
    for label, scores in report.items():
        if isinstance(scores, dict) and label not in ['accuracy', 'macro avg', 'weighted avg']:
            metrics_data[label] = {
                'Precision': scores.get('precision', 0),
                'Recall': scores.get('recall', 0),
                'F1-score': scores.get('f1-score', 0)
            }

    if metrics_data:
        metrics_df = pd.DataFrame(metrics_data).T
        metrics_df.plot(kind='bar', figsize=(10, 7))
        plt.title('Precision, Recall, and F1-score per Class')
        plt.ylabel('Score')
        plt.xticks(rotation=45, ha='right')
        plt.legend(loc='lower right')
        plt.tight_layout()
        plt.show()
    else:
        print("No per-class metrics available for visualization.")


    # 6. ROC Curve and AUC Score
    print("\nROC Curve and AUC Score:")
    # Need to load the ensemble pipeline again to get the meta-model's classes for plotting
    final_model_path = '/content/drive/MyDrive/TikTok_Tech_Jam/models/final_production_ensemble_model.pkl'
    try:
        ensemble_pipeline = joblib.load(final_model_path)
        meta_model_classes = ensemble_pipeline['final_meta_model'].classes_
    except FileNotFoundError:
        print(f"Error: Trained model not found at {final_model_path}. Cannot plot ROC curves with correct class labels.")
        meta_model_classes = sorted(list(set(y_test) | set(y_pred))) # Fallback to available labels

    try:
        # Binarize the true labels for multi-class ROC using the meta-model's classes
        # This ensures the order matches y_prob
        y_test_bin = label_binarize(y_test, classes=meta_model_classes)
        n_classes = y_test_bin.shape[1]

        # Compute ROC curve and ROC area for each class
        fpr = dict()
        tpr = dict()
        roc_auc = dict()
        for i in range(n_classes):
             # Ensure y_prob has the correct number of columns matching binarized labels
             if y_prob.shape[1] > i:
                  fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
                  roc_auc[i] = auc(fpr[i], tpr[i])
             else:
                  print(f"Warning: y_prob does not have a column for class index {i} ({meta_model_classes[i]}). Skipping ROC for this class.")
                  roc_auc[i] = np.nan # Assign NaN if data is missing


        # Plot ROC curves
        plt.figure(figsize=(10, 8))
        colors = plt.cm.get_cmap('tab10', n_classes) # Use a colormap for different classes
        for i, label in enumerate(meta_model_classes):
         if i in fpr and i in tpr and not np.isnan(roc_auc[i]): # Only plot if ROC data was computed and not NaN
             plt.plot(fpr[i], tpr[i], color=colors(i), lw=2,
                     label=f'ROC curve of class {label} (area = {roc_auc[i]:.2f})')

        plt.plot([0, 1], [0, 1], 'k--', lw=2)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver Operating Characteristic (ROC) Curve - Multi-class (One-vs-Rest)')
        plt.legend(loc="lower right")
        plt.show()

        print("\nAUC Scores per Class (One-vs-Rest):")
        for i, label in enumerate(meta_model_classes):
            if i in roc_auc and not np.isnan(roc_auc[i]):
                 print(f"  Class {label}: {roc_auc[i]:.4f}")
            elif i in roc_auc and np.isnan(roc_auc[i]):
                 print(f"  Class {label}: AUC not computable (e.g., only one class present in test data or issues with y_prob)")

    except ValueError as e:
        print(f"\nCould not compute ROC curve and AUC due to a value error: {e}. This can happen with inconsistent numbers of classes or samples.")
    except Exception as e:
        print(f"\nAn unexpected error occurred during ROC curve and AUC computation: {e}")

